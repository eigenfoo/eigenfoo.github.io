<!doctype html><html lang=en-us><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://www.georgeho.org/favicon.ico><title>Floating-Point Formats and Deep Learning | George Ho</title><meta name=title content="Floating-Point Formats and Deep Learning"><meta name=description content="Floating-point formats are not the most glamorous or (frankly) the important consideration when working with deep learning models: if your model isn&rsquo;t working well, then your floating-point format certainly isn&rsquo;t going to save you! However, past a certain point of model complexity/model size/training time, your choice of floating-point format can have a significant impact on your model training times and even performance.
Here&rsquo;s how the rest of this post is structured:"><meta name=keywords content="deep-learning,machine-learning,"><meta property="og:title" content="Floating-Point Formats and Deep Learning"><meta property="og:description" content="Floating-point formats are not the most glamorous or (frankly) the important consideration when working with deep learning models: if your model isn&rsquo;t working well, then your floating-point format certainly isn&rsquo;t going to save you! However, past a certain point of model complexity/model size/training time, your choice of floating-point format can have a significant impact on your model training times and even performance.
Here&rsquo;s how the rest of this post is structured:"><meta property="og:type" content="article"><meta property="og:url" content="https://www.georgeho.org/floating-point-deep-learning/"><meta property="og:image" content="https://www.georgeho.org/assets/images/asterism.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2020-07-26T00:00:00+00:00"><meta property="article:modified_time" content="2020-07-26T00:00:00+00:00"><meta property="og:site_name" content="⁂ George Ho"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.georgeho.org/assets/images/asterism.png"><meta name=twitter:title content="Floating-Point Formats and Deep Learning"><meta name=twitter:description content="Floating-point formats are not the most glamorous or (frankly) the important consideration when working with deep learning models: if your model isn&rsquo;t working well, then your floating-point format certainly isn&rsquo;t going to save you! However, past a certain point of model complexity/model size/training time, your choice of floating-point format can have a significant impact on your model training times and even performance.
Here&rsquo;s how the rest of this post is structured:"><meta itemprop=name content="Floating-Point Formats and Deep Learning"><meta itemprop=description content="Floating-point formats are not the most glamorous or (frankly) the important consideration when working with deep learning models: if your model isn&rsquo;t working well, then your floating-point format certainly isn&rsquo;t going to save you! However, past a certain point of model complexity/model size/training time, your choice of floating-point format can have a significant impact on your model training times and even performance.
Here&rsquo;s how the rest of this post is structured:"><meta itemprop=datePublished content="2020-07-26T00:00:00+00:00"><meta itemprop=dateModified content="2020-07-26T00:00:00+00:00"><meta itemprop=wordCount content="1542"><meta itemprop=image content="https://www.georgeho.org/assets/images/asterism.png"><meta itemprop=keywords content="deep-learning,machine-learning,"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({ tex2jax: {inlineMath: [["$", "$"]]} })
</script><script async src=//static.getclicky.com/101349618.js></script><noscript><p><img alt=Clicky width=1 height=1 src=//in.getclicky.com/101349618ns.gif></p></noscript><link rel=stylesheet media=all href=/assets/fonts/nicholson-gothic.css type=text/css><link rel=stylesheet media=all href=/assets/fonts/triplicate-a.css type=text/css><script type=text/javascript>navigator.appVersion.indexOf("Win")!=-1?document.write('<link rel="stylesheet" type="text/css" media="all" href="/assets/fonts/equity-a.css"/>'):navigator.appVersion.indexOf("Mac")!=-1?navigator.userAgent.match(/iPad/i)!=null?(document.write('<link rel="stylesheet" media="only screen and (max-device-width: 1024px)" href="/assets/fonts/equity-b.css" type="text/css"/>'),document.write('<link rel="stylesheet" media="only screen and (min-device-width: 768px) and (max-device-width: 1024px) and (-webkit-min-device-pixel-ratio: 2)" type="text/css" href="/assets/fonts/equity-a.css"/>')):document.write('<link rel="stylesheet" type="text/css" media="all" href="/assets/fonts/equity-b.css"/>'):document.write('<link rel="stylesheet" type="text/css" media="all" href="/assets/fonts/equity-a.css"/>')</script><style>html{font-size:18px;font-size:min(max(18px,4vw),26px)}content{line-height:1.45}body{max-width:800px;font-family:Equity,times new roman,Palatino,serif;background-color:#fffff8}code{font-family:Triplicate,lucida console,monospace;font-size:85%;background-color:unset}pre code{white-space:pre;overflow-x:auto;font-size:14px;font-size:min(max(12px,2vw),16px);text-size-adjust:100%;-ms-text-size-adjust:100%;-moz-text-size-adjust:100%;-webkit-text-size-adjust:100%}h1,h2,h3,h4,h5,h6{font-family:NicholsonGothic,Verdana,sans-serif;line-height:1.25}h2,h3,h4,h5,h6{margin-top:8%;margin-bottom:-1%}nav{margin-top:3%;margin-bottom:5%}ul.blog-posts li span{flex:0 0 140px}input[name=email]{font-family:Triplicate,lucida console,monospace;font-size:80%}input[type=submit]{font-family:Equity,times new roman,Palatino,serif;font-size:80%}@media(prefers-color-scheme:dark){body{background-color:#111}}</style></head><body><header><a href=/ class=title><h2>⁂ George Ho</h2></a><nav><a href=/>Home</a>
<a href=/blog>Blog</a>
<a href=/crosswords/>Crosswords</a>
<a href=/work/>Work</a></nav></header><main><h1>Floating-Point Formats and Deep Learning</h1><p><i><time datetime=2020-07-26 pubdate>2020-07-26</time></i></p><content><p>Floating-point formats are not the most glamorous or (frankly) the important
consideration when working with deep learning models: if your model isn&rsquo;t working well,
then your floating-point format certainly isn&rsquo;t going to save you! However, past a
certain point of model complexity/model size/training time, your choice of
floating-point format can have a significant impact on your model training times and
even performance.</p><p>Here&rsquo;s how the rest of this post is structured:</p><ol><li><a href=#floating-point-in-_my_-deep-learning>Why should you, a deep learning practitioner,
care</a> about what floating-point format your
model uses?</li><li><a href=#floating-point-formats>What even <em>is</em> floating-point</a>, especially these new
floating-point formats made specifically for deep learning?</li><li><a href=#advice-for-practitioners>What practical advice is there</a> on using floating-point
formats for deep learning?</li></ol><h2 id=floating-point-in-_my_-deep-learning>Floating-Point? In <em>My</em> Deep Learning?</h2><p><a href=https://knowyourmeme.com/photos/6052-its-more-likely-than-you-think>It&rsquo;s more likely than you
think!</a></p><p>It&rsquo;s been known for quite some time that <a href=https://arxiv.org/abs/1502.02551>deep neural networks can
tolerate</a> <a href=https://arxiv.org/abs/1412.7024>lower numerical
precision</a>. High-precision calculations turn out not
to be that useful in training or inferencing neural networks: the additional precision
confers no benefit while being slower and less memory-efficient.</p><p>Surprisingly, some models can even reach a higher accuracy with lower precision, which
recent research attributes to the <a href=https://arxiv.org/abs/1809.00095>regularization effects from the lower
precision</a>.</p><p>Finally (and this is speculation on my part — I haven&rsquo;t seen any experiments or papers
corroborating this), it&rsquo;s possible that certain complicated models <em>cannot converge</em>
unless you use an appropriately precise format. There&rsquo;s a drift between the analytical
gradient update and what the actual backward propagation looks like: the lower the
precision, the bigger the drift. I&rsquo;d expect that deep learning is particularly
susceptible to an issue here because there&rsquo;s a lot of multiplications, divisions and
reduction operations.</p><h2 id=floating-point-formats>Floating-Point Formats</h2><p>Let&rsquo;s take a quick look at three floating-point formats for deep learning. There are a
lot more floating-point formats, but only a few have gained traction: floating-point
formats require the appropriate hardware and firmware support, which restricts the
introduction and adoption of new formats.</p><p>For a quick overview, Grigory Sapunov wrote a great <a href=https://medium.com/@moocaholic/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407>run-down of various floating-point
formats for deep
learning</a>.</p><h3 id=ieee-floating-point-formats>IEEE floating-point formats</h3><p>These floating-point formats are probably what most people think of when someone says
&ldquo;floating-point&rdquo;. The IEEE standard 754 sets out several formats, but for the purposes
of deep learning we are only interested three:
<a href=https://en.wikipedia.org/wiki/Half-precision_floating-point_format>FP16</a>,
<a href=https://en.wikipedia.org/wiki/Single-precision_floating-point_format>FP32</a> and
<a href=https://en.wikipedia.org/wiki/Double-precision_floating-point_format>FP64</a> (a.k.a.
half-, single- and double-precision floating-point formats)<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><p>Let&rsquo;s take FP32 as an example. Each FP32 number is a sequence of 32 bits,
$b_{31} b_{30} &mldr; b_{0}$. Altogether, this sequence represents the real number</p><p>$$ (-1)^{b_{31}} \cdot 2^{(b_{30} b_{29} &mldr; b_{23}) - 127} \cdot (1.b_{22} b_{21} &mldr; b_{0})_2 $$</p><p>Here, $b_{31}$ (the <em>sign bit</em>) determines the sign of the represented value.</p><p>$b_{30}$ through $b_{23}$ determine the magnitude or scale of the represented value
(notice that a change in any of these bits drastically changes the size of the
represented value). These bits are called the <em>exponent</em> or <em>scale bits</em>.</p><p>Finally, $b_{22}$ through $b_{0}$ determine the precise value of the represented
value. These bits are called the <em>mantissa</em> or <em>precision bits</em>.</p><p>Obviously, the more bits you have, the more you can do. Here&rsquo;s how the three formats
break down:</p><table><thead><tr><th style=text-align:left></th><th style=text-align:right>Sign Bits</th><th style=text-align:right>Exponent (Scale) Bits</th><th style=text-align:right>Mantissa (Precision) Bits</th></tr></thead><tbody><tr><td style=text-align:left>FP16</td><td style=text-align:right>1</td><td style=text-align:right>5</td><td style=text-align:right>10</td></tr><tr><td style=text-align:left>FP32</td><td style=text-align:right>1</td><td style=text-align:right>8</td><td style=text-align:right>23</td></tr><tr><td style=text-align:left>FP64</td><td style=text-align:right>1</td><td style=text-align:right>11</td><td style=text-align:right>53</td></tr></tbody></table><p>There are some details that I&rsquo;m leaving out here (e.g. how to represent NaNs, positive
and negative infinities), but this is largely how floating point numbers work. A lot
more detail can be found on the <a href=https://en.wikipedia.org/wiki/Floating-point_arithmetic#IEEE_754:_floating_point_in_modern_computers>Wikipedia
page</a>
and of course the <a href=https://ieeexplore.ieee.org/document/8766229>latest revision of the IEEE standard
754</a> itself.</p><p>FP32 and FP64 are widely supported by both software (C/C++, PyTorch, TensorFlow) and
hardware (x86 CPUs and most NVIDIA/AMD GPUs).</p><p>FP16, on the other hand, is not as widely supported in software (you need to use <a href=http://half.sourceforge.net/>a
special library</a> to use them in C/C++). However, since
deep learning is trending towards favoring FP16 over FP32, it has found support in the
main deep learning frameworks (e.g. <code>tf.float16</code> and <code>torch.float16</code>). In terms of
hardware, FP16 is not supported in x86 CPUs as a distinct type, but is well-supported on
modern GPUs.</p><h3 id=google-bfloat16>Google BFloat16</h3><p>BFloat16 (a.k.a. the Brain Floating-Point Format, after Google Brain) is basically the
same as FP16, but 3 mantissa bits become exponent bits (i.e. bfloat16 trades 3 bits'
worth of precision for scale).</p><figure class=align-center><img style=float:middle src=/assets/images/bfloat16.png alt="Diagram illustrating the number and type of bits in bfloat16."><figcaption>The number and type of bits in bfloat16. Source: <a href=https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus>Google Cloud blog</a>.</figcaption></figure><p>When it comes to deep learning, there are generally three &ldquo;flavors&rdquo; of values: weights,
activations and gradients. Google suggests storing weights and gradients in FP32, and
storing activations in bfloat16. However, in particularly gracious circumstances,
weights can be stored in bfloat16 without a significant performance degradation.</p><p>You can read a lot more on the <a href=https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus>Google Cloud
blog</a>,
and <a href=https://arxiv.org/abs/1905.12322>this paper by Intel and Facebook studying the bfloat16
format</a>.</p><p>In terms of software support, bfloat16 is not supported in C/C++, but is supported in
TensorFlow (<a href=https://www.tensorflow.org/api_docs/python/tf#bfloat16><code>tf.bfloat16</code></a>) and
PyTorch (<a href=https://www.tensorflow.org/api_docs/python/tf#bfloat16><code>torch.bfloat16</code></a>).</p><p>In terms of hardware support, it is supported by <a href=https://en.wikipedia.org/wiki/Cooper_Lake_(microarchitecture)>some modern
CPUS</a>, but the real
support comes out in GPUs and ASICs. At the time of writing, bfloat16 is supported by
the NVIDIA A100 (the first GPU to support it!), and <a href=https://www.techpowerup.com/260344/future-amd-gpu-architecture-to-implement-bfloat16-hardware>will be supported in future AMD
GPUs</a>.
And of course, it is supported by Google TPU v2/v3.</p><h3 id=nvidia-tensorfloat>NVIDIA TensorFloat</h3><p>Strictly speaking, this isn&rsquo;t really its own floating-point format, just an overzealous
branding of the technique that NVIDIA developed to train in mixed precision on their
Tensor Core hardware<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><p>An NVIDIA TensorFloat (a.k.a. TF32) is just a 32-bit float that drops 13 precision bits
in order to execute on Tensor Cores. Thus, it has the precision of FP16 (10 bits), with
the range of FP32 (8 bits). However, if you&rsquo;re not using Tensor Cores, it&rsquo;s just a
32-bit float; if you&rsquo;re only thinking about storage, it&rsquo;s just a 32-bit float.</p><figure class=align-center><img style=float:middle src=/assets/images/tensorfloat32.png alt="Diagram illustrating the number and type of bits in an NVIDIA TensorFloat"><figcaption>The number and type of bits in an NVIDIA TensorFloat. Source: <a href=https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/>NVIDIA blog</a>.</figcaption></figure><p>One distinct advantage of TF32 is that they&rsquo;re kind of like FP32. To quote from the
NVIDIA developer blog,</p><blockquote><p>Applications using NVIDIA libraries enable users to harness the benefits of TF32 with no
code change required. TF32 Tensor Cores operate on FP32 inputs and produce results in
FP32. Non-matrix operations continue to use FP32.</p></blockquote><p>You can read more about TF32 <a href=https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/>on the NVIDIA
blog</a>, and
about its hardware support in the Ampere architecture on <a href=https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/>the NVIDIA developer
blog</a>.</p><p>TF32 is not in the C/C++ standard at all, but is supported in <a href=https://developer.nvidia.com/blog/cuda-11-features-revealed/>CUDA
11</a>.</p><p>Hardware-wise, the NVIDIA A100 is the first GPU (and, at the time of writing, the only
device) supporting TF32.</p><h2 id=advice-for-practitioners>Advice for Practitioners</h2><p>The first thing to say is that floating-point formats are <em>by no means</em> the most
important consideration for your deep learning model — not even close. Floating-point
formats will most likely only make a difference for very large or complex models, for
which fitting the model on GPU memory is a challenge, or for which training times are
excruciatingly long.</p><p>The second thing to say is that any practical advice has to be heavily dependent on what
hardware you have available to you.</p><h3 id=automatic-mixed-precision-amp-training--a-good-default>Automatic mixed precision (AMP) training — a good default</h3><p>Most deep learning stacks support mixed-precision training, which is a pretty good
default option to reap some of the benefits of low-precision training, while still
reasonably avoiding underflow and overflow problems.</p><p>TensorFlow supports <a href=https://www.tensorflow.org/guide/mixed_precision>mixed-precision training
natively</a>, whereas the <a href=https://github.com/NVIDIA/apex>NVIDIA Apex
library</a> makes automatic mixed precision training
available in PyTorch. To get started, take a look at NVIDIA&rsquo;s <a href=https://developer.nvidia.com/automatic-mixed-precision>developer guide for
AMP</a>, and <a href=https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html>documentation for
training in mixed
precision</a>.</p><p>It&rsquo;s worth going over the gist of mixed precision training. There are basically two main
tricks:</p><ol><li><em>Loss scaling:</em> multiply the loss by some large number, and divide the gradient
updates by this same large number. This avoids the loss underflowing (i.e. clamping
to zero because of the finite precision) in FP16, while still maintaining faithful
backward propagation.</li><li><em>FP32 master copy of weights</em>: store the weights themselves in FP32, but cast them to
FP16 before doing the forward and backward propagation (to reap the performance
benefits). During the weight update, the FP16 gradients are cast to FP32 to update
the master copy.</li></ol><p>You can read more about these techniques in <a href=https://arxiv.org/abs/1710.03740>this paper by NVIDIA and Baidu
Research</a>, or on the accompanying <a href=https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/>blog post by
NVIDIA</a>.</p><h3 id=alternative-floating-point-formats--make-sure-itll-be-worth-it>Alternative floating-point formats — make sure it&rsquo;ll be worth it</h3><p>If you&rsquo;ve already trained your model in mixed precision, it might not be worth the time
or effort to port your code to take advantage of an alternative floating-point format
and bleeding edge hardware.</p><p>However, if you choose to go that route, make sure your use case really demands it.
Perhaps you can&rsquo;t scale up your model without using bfloat16, or you really need to cut
down on training times.</p><p>Unfortunately, I don&rsquo;t have a well-informed opinion on how bfloat16 stacks up against
TF32, so &ldquo;do your homework&rdquo; is all I can advise. However, since the NVIDIA A100s only
just (at the time of writing) dropped into the market, it&rsquo;ll be interesting to see what
the machine learning community thinks of the various low precision options available.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Technically speaking, there are <a href=https://en.wikipedia.org/wiki/Quadruple-precision_floating-point_format>quadruple-</a> and <a href=https://en.wikipedia.org/wiki/Octuple-precision_floating-point_format>octuple-precision</a> floating-point formats, but those are pretty rarely used, and certainly unheard of in deep learning.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>A Tensor Core is essentially a mixed-precision FP16/FP32 core, which NVIDIA has optimized for deep learning applications.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></content><p><a href=https://www.georgeho.org/blog/deep-learning/>#deep-learning</a>
<a href=https://www.georgeho.org/blog/machine-learning/>#machine-learning</a></p></main><footer><form action=https://buttondown.email/api/emails/embed-subscribe/eigenfoo method=post target=popupwindow onsubmit='window.open("https://buttondown.email/eigenfoo","popupwindow")' class=embeddable-buttondown-form><label for=bd-email>Get my (monthly?) blog posts over email:</label><br><input type=email name=email id=bd-email placeholder=your@email.com>
<input type=submit value=Subscribe></form></footer></body></html>