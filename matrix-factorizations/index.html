<!doctype html><html lang=en-us><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://www.georgeho.org/favicon.ico><title>Probabilistic and Bayesian Matrix Factorizations for Text Clustering | George Ho</title><meta name=title content="Probabilistic and Bayesian Matrix Factorizations for Text Clustering"><meta name=description content="Natural language processing is in a curious place right now. It was always a late bloomer (as far as machine learning subfields go), and it&rsquo;s not immediately obvious how close the field is to viable, large-scale, production-ready techniques (in the same way that, say, computer vision is). For example, Sebastian Ruder predicted that the field is close to a watershed moment, and that soon we&rsquo;ll have downloadable language models. However, Ana Marasović points out that there is a tremendous amount of work demonstrating that:"><meta name=keywords content="bayes,natural-language-processing,talks,"><meta property="og:title" content="Probabilistic and Bayesian Matrix Factorizations for Text Clustering"><meta property="og:description" content="Natural language processing is in a curious place right now. It was always a late bloomer (as far as machine learning subfields go), and it&rsquo;s not immediately obvious how close the field is to viable, large-scale, production-ready techniques (in the same way that, say, computer vision is). For example, Sebastian Ruder predicted that the field is close to a watershed moment, and that soon we&rsquo;ll have downloadable language models. However, Ana Marasović points out that there is a tremendous amount of work demonstrating that:"><meta property="og:type" content="article"><meta property="og:url" content="https://www.georgeho.org/matrix-factorizations/"><meta property="og:image" content="https://www.georgeho.org/assets/images/asterism.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2018-10-13T00:00:00+00:00"><meta property="article:modified_time" content="2018-10-13T00:00:00+00:00"><meta property="og:site_name" content="⁂ George Ho"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.georgeho.org/assets/images/asterism.png"><meta name=twitter:title content="Probabilistic and Bayesian Matrix Factorizations for Text Clustering"><meta name=twitter:description content="Natural language processing is in a curious place right now. It was always a late bloomer (as far as machine learning subfields go), and it&rsquo;s not immediately obvious how close the field is to viable, large-scale, production-ready techniques (in the same way that, say, computer vision is). For example, Sebastian Ruder predicted that the field is close to a watershed moment, and that soon we&rsquo;ll have downloadable language models. However, Ana Marasović points out that there is a tremendous amount of work demonstrating that:"><meta itemprop=name content="Probabilistic and Bayesian Matrix Factorizations for Text Clustering"><meta itemprop=description content="Natural language processing is in a curious place right now. It was always a late bloomer (as far as machine learning subfields go), and it&rsquo;s not immediately obvious how close the field is to viable, large-scale, production-ready techniques (in the same way that, say, computer vision is). For example, Sebastian Ruder predicted that the field is close to a watershed moment, and that soon we&rsquo;ll have downloadable language models. However, Ana Marasović points out that there is a tremendous amount of work demonstrating that:"><meta itemprop=datePublished content="2018-10-13T00:00:00+00:00"><meta itemprop=dateModified content="2018-10-13T00:00:00+00:00"><meta itemprop=wordCount content="1181"><meta itemprop=image content="https://www.georgeho.org/assets/images/asterism.png"><meta itemprop=keywords content="bayes,natural-language-processing,talks,"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({ tex2jax: {inlineMath: [["$", "$"]]} })
</script><script async src=//static.getclicky.com/101349618.js></script><noscript><p><img alt=Clicky width=1 height=1 src=//in.getclicky.com/101349618ns.gif></p></noscript><link rel=stylesheet media=all href=/assets/fonts/nicholson-gothic.css type=text/css><link rel=stylesheet media=all href=/assets/fonts/triplicate-a.css type=text/css><script type=text/javascript>navigator.appVersion.indexOf("Win")!=-1?document.write('<link rel="stylesheet" type="text/css" media="all" href="/assets/fonts/equity-a.css"/>'):navigator.appVersion.indexOf("Mac")!=-1?navigator.userAgent.match(/iPad/i)!=null?(document.write('<link rel="stylesheet" media="only screen and (max-device-width: 1024px)" href="/assets/fonts/equity-b.css" type="text/css"/>'),document.write('<link rel="stylesheet" media="only screen and (min-device-width: 768px) and (max-device-width: 1024px) and (-webkit-min-device-pixel-ratio: 2)" type="text/css" href="/assets/fonts/equity-a.css"/>')):document.write('<link rel="stylesheet" type="text/css" media="all" href="/assets/fonts/equity-b.css"/>'):document.write('<link rel="stylesheet" type="text/css" media="all" href="/assets/fonts/equity-a.css"/>')</script><style>html{font-size:18px;font-size:min(max(18px,4vw),22px)}content{line-height:1.45}body{font-family:Equity,Georgia,serif;background-color:#fffff8}code{font-family:Triplicate,lucida console,monospace;font-size:85%;background-color:unset}pre code{white-space:pre;overflow-x:auto;font-size:14px;font-size:min(max(12px,2vw),14px);text-size-adjust:100%;-ms-text-size-adjust:100%;-moz-text-size-adjust:100%;-webkit-text-size-adjust:100%}h1,h2,h3,h4,h5,h6{font-family:NicholsonGothic,Verdana,sans-serif;line-height:1.25}h2,h3,h4,h5,h6{margin-top:8%;margin-bottom:-1%}nav{margin-top:3%;margin-bottom:5%}@media(prefers-color-scheme:dark){body{background-color:#111}}</style></head><body><header><a href=/ class=title><h2>⁂ George Ho</h2></a><nav><a href=/>Home</a>
<a href=/work/>Work</a>
<a href=/blog>Blog</a></nav></header><main><h1>Probabilistic and Bayesian Matrix Factorizations for Text Clustering</h1><p><i><time datetime=2018-10-13 pubdate>2018-10-13</time></i></p><content><p>Natural language processing is in a curious place right now. It was always a
late bloomer (as far as machine learning subfields go), and it&rsquo;s not immediately
obvious how close the field is to viable, large-scale, production-ready
techniques (in the same way that, say, <a href=https://clarifai.com/models/>computer vision
is</a>). For example, <a href=https://ruder.io>Sebastian
Ruder</a> predicted that the field is <a href=https://thegradient.pub/nlp-imagenet/>close to a watershed
moment</a>, and that soon we&rsquo;ll have
downloadable language models. However, <a href=https://thegradient.pub/author/ana/>Ana
Marasović</a> points out that there is <a href=https://thegradient.pub/frontiers-of-generalization-in-natural-language-processing/>a
tremendous amount of work demonstrating
that</a>:</p><blockquote><p>“despite good performance on benchmark datasets, modern NLP techniques are
nowhere near the skill of humans at language understanding and reasoning when
making sense of novel natural language inputs”.</p></blockquote><p>I am confident that I am <em>very</em> bad at making lofty predictions about the
future. Instead, I&rsquo;ll talk about something I know a bit about: simple solutions
to concrete problems, with some Bayesianism thrown in for good measure
:grinning:.</p><p>This blog post summarizes some literature on probabilistic and Bayesian
matrix factorization methods, keeping an eye out for applications to one
specific task in NLP: text clustering. It&rsquo;s exactly what it sounds like, and
there&rsquo;s been a fair amount of success in applying text clustering to many other
NLP tasks (e.g. check out these examples in <a href=https://www-users.cs.umn.edu/~hanxx023/dmclass/scatter.pdf>document
organization</a>,
<a href=http://jmlr.csail.mit.edu/papers/volume3/bekkerman03a/bekkerman03a.pdf>corpus</a>
<a href=https://www.cs.technion.ac.il/~rani/el-yaniv-papers/BekkermanETW01.pdf>summarization</a>
and <a href=http://www.kamalnigam.com/papers/emcat-aaai98.pdf>document
classification</a>).</p><p>What follows is a literature review of three matrix factorization techniques for
machine learning: one classical, one probabilistic and one Bayesian. I also
experimented with applying these methods to text clustering: I gave a guest
lecture on my results to a graduate-level machine learning class at The Cooper
Union (the slide deck is below). Dive in!</p><h2 id=non-negative-matrix-factorization-nmf>Non-Negative Matrix Factorization (NMF)</h2><p>NMF is a <a href=https://en.wikipedia.org/wiki/Non-negative_matrix_factorization>very
well-known</a>
<a href=http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html>matrix
factorization</a>
<a href=https://arxiv.org/abs/1401.5226>technique</a>, perhaps most famous for its
applications in <a href=http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/>collaborative filtering and the Netflix
Prize</a>.</p><p>Factorize your (entrywise non-negative) $m \times n$ matrix $V$ as
$V = WH$, where $W$ is $m \times p$ and $H$ is $p \times n$. $p$
is the dimensionality of your latent space, and each latent dimension usually
comes to quantify something with semantic meaning. There are several algorithms
to compute this factorization, but Lee and Seung&rsquo;s <a href="https://dl.acm.org/citation.cfm?id=3008829">multiplicative update
rule</a> (originally published in NIPS
2000) is most popular.</p><p>Fairly simple: enough said, I think.</p><h2 id=probabilistic-matrix-factorization-pmf>Probabilistic Matrix Factorization (PMF)</h2><p>Originally introduced as a paper at <a href=https://papers.nips.cc/paper/3208-probabilistic-matrix-factorization>NIPS
2007</a>,
<em>probabilistic matrix factorization</em> is essentially the exact same model as NMF,
but with uncorrelated (a.k.a. “spherical”) multivariate Gaussian priors placed
on the rows and columns of $U$ and $V$. Expressed as a graphical model, PMF
would look like this:</p><figure><a href=/assets/images/pmf.png><img style=float:middle src=/assets/images/pmf.png alt="Graphical model (using plate notation) for probabilistic matrix factorization (PMF)"></a></figure><p>Note that the priors are placed on the <em>rows</em> of the $U$ and $V$ matrices.</p><p>The authors then (somewhat disappointing) proceed to find the MAP estimate of
the $U$ and $V$ matrices. They show that maximizing the posterior is
equivalent to minimizing the sum-of-squared-errors loss function with two
quadratic regularization terms:</p><p>$$
\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{M} {I_{ij} (R_{ij} - U_i^T V_j)^2} +
\frac{\lambda_U}{2} \sum_{i=1}^{N} |U|_{Fro}^2 +
\frac{\lambda_V}{2} \sum_{j=1}^{M} |V|_{Fro}^2
$$</p><p>where $|\cdot|_{Fro}$ denotes the Frobenius norm, and $I_{ij}$ is 1 if document
$i$ contains word $j$, and 0 otherwise.</p><p>This loss function can be minimized via gradient descent, and implemented in
your favorite deep learning framework (e.g. Tensorflow or PyTorch).</p><p>The problem with this approach is that while the MAP estimate is often a
reasonable point in low dimensions, it becomes very strange in high dimensions,
and is usually not informative or special in any way. Read <a href=https://www.inference.vc/high-dimensional-gaussian-distributions-are-soap-bubble/>Ferenc Huszár’s blog
post</a>
for more.</p><h2 id=bayesian-probabilistic-matrix-factorization-bpmf>Bayesian Probabilistic Matrix Factorization (BPMF)</h2><p>Strictly speaking, PMF is not a Bayesian model. After all, there aren&rsquo;t any
priors or posteriors, only fixed hyperparameters and a MAP estimate. <em>Bayesian
probabilistic matrix factorization</em>, originally published by <a href="https://dl.acm.org/citation.cfm?id=1390267">researchers from
the University of Toronto</a> is a
fully Bayesian treatment of PMF.</p><p>Instead of saying that the rows/columns of U and V are normally distributed with
zero mean and some precision matrix, we place hyperpriors on the mean vector and
precision matrices. The specific priors are Wishart priors on the covariance
matrices (with scale matrix $W_0$ and $\nu_0$ degrees of freedom), and
Gaussian priors on the means (with mean $\mu_0$ and covariance equal to the
covariance given by the Wishart prior). Expressed as a graphical model, BPMF
would look like this:</p><figure><a href=/assets/images/bpmf.png><img style=float:middle src=/assets/images/bpmf.png alt="Graphical model (using plate notation) for Bayesian probabilistic matrix factorization (BPMF)"></a></figure><p>Note that, as above, the priors are placed on the <em>rows</em> of the $U$ and $V$
matrices, and that $n$ is the dimensionality of latent space (i.e. the number
of latent dimensions in the factorization).</p><p>The authors then sample from the posterior distribution of $U$ and $V$ using
a Gibbs sampler. Sampling takes several hours: somewhere between 5 to 180,
depending on how many samples you want. Nevertheless, the authors demonstrate
that BPMF can achieve more accurate and more robust results on the Netflix data
set.</p><p>I would propose two changes to the original paper:</p><ol><li>Use an LKJ prior on the covariance matrices instead of a Wishart prior.
<a href=https://docs.pymc.io/notebooks/LKJ.html>According to Michael Betancourt and the PyMC3 docs, this is more numerically
stable</a>, and will lead to better
inference.</li><li>Use a more robust sampler such as NUTS (instead of a Gibbs sampler), or even
resort to variational inference. The paper makes it clear that BPMF is a
computationally painful endeavor, so any speedup to the method would be a
great help. It seems to me that for practical real-world applications to
collaborative filtering, we would want to use variational inference. Netflix
ain&rsquo;t waiting 5 hours for their recommendations.</li></ol><h2 id=application-to-text-clustering>Application to Text Clustering</h2><p>Most of the work in these matrix factorization techniques focus on
dimensionality reduction: that is, the problem of finding two factor matrices
that faithfully reconstruct the original matrix when multiplied together.
However, I was interested in applying the exact same techniques to a separate
task: text clustering.</p><p>A natural question is: why is matrix factorization<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> a good technique to use
for text clustering? Because it is simultaneously a clustering and a feature
engineering technique: not only does it offer us a latent representation of the
original data, but it also gives us a way to easily <em>reconstruct</em> the original
data from the latent variables! This is something that <a href=https://www.georgeho.org/lda-sucks>latent Dirichlet
allocation</a>, for instance, cannot do.</p><p>Matrix factorization lives an interesting double life: clustering technique by
day, feature transformation technique by night. <a href=http://charuaggarwal.net/text-cluster.pdf>Aggarwal and
Zhai</a> suggest that chaining matrix
factorization with some other clustering technique (e.g. agglomerative
clustering or topic modelling) is common practice and is called <em>concept
decomposition</em>, but I haven&rsquo;t seen any other source back this up.</p><p>I experimented with using these techniques to cluster subreddits (<a href=https://www.georgeho.org/reddit-clusters>sound
familiar?</a>). In a nutshell, nothing seemed
to work out very well, and I opine on why I think that&rsquo;s the case in the slide
deck below. This talk was delivered to a graduate-level course in frequentist
machine learning.</p><blockquote class=embedly-card><h4><a href=https://speakerdeck.com/_eigenfoo/probabilistic-and-bayesian-matrix-factorizations-for-text-clustering>Probabilistic and Bayesian Matrix Factorizations for Text Clustering</a></h4><p>I experimented with using these techniques to cluster subreddits. In a nutshell, nothing seemed to work out very well, and I opine on why I think that’s the case in this slide deck. This talk was delivered to a graduate-level course in frequentist machine learning.</p></blockquote><script async src=//cdn.embedly.com/widgets/platform.js></script><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>which is, by the way, a <a href=http://scikit-learn.org/stable/modules/decomposition.html>severely underappreciated technique in machine
learning</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></content><p><a href=https://www.georgeho.org/blog/bayes/>#bayes</a>
<a href=https://www.georgeho.org/blog/natural-language-processing/>#natural-language-processing</a>
<a href=https://www.georgeho.org/blog/talks/>#talks</a></p></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>