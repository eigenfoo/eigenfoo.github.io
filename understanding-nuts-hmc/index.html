<!doctype html><html lang=en-US><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://www.georgeho.org/favicon.ico><title>Understanding NUTS and HMC | George Ho</title>
<meta name=title content="Understanding NUTS and HMC"><meta name=description content="&ldquo;Bayesian modeling is harder than deep learning&rdquo; is a sentiment I&rsquo;ve been hearing a lot lately. While I&rsquo;m skeptical of sweeping statements like that, I agree when it comes to the central inference algorithm &mdash; how MCMC samplers work (especially the de facto standard samplers, NUTS and HMC) is one of the most difficult concepts I&rsquo;ve tried to learn, and is certainly harder than autodifferentiation or backpropagation.
So I thought I&rsquo;d share what worked for me when I tried to teach myself NUTS and HMC."><meta name=keywords content="bayes,"><meta property="og:title" content="Understanding NUTS and HMC"><meta property="og:description" content="&ldquo;Bayesian modeling is harder than deep learning&rdquo; is a sentiment I&rsquo;ve been hearing a lot lately. While I&rsquo;m skeptical of sweeping statements like that, I agree when it comes to the central inference algorithm &mdash; how MCMC samplers work (especially the de facto standard samplers, NUTS and HMC) is one of the most difficult concepts I&rsquo;ve tried to learn, and is certainly harder than autodifferentiation or backpropagation.
So I thought I&rsquo;d share what worked for me when I tried to teach myself NUTS and HMC."><meta property="og:type" content="article"><meta property="og:url" content="https://www.georgeho.org/understanding-nuts-hmc/"><meta property="og:image" content="https://www.georgeho.org/assets/images/asterism.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2021-01-07T00:00:00+00:00"><meta property="article:modified_time" content="2021-01-07T00:00:00+00:00"><meta property="og:site_name" content="⁂ George Ho"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.georgeho.org/assets/images/asterism.png"><meta name=twitter:title content="Understanding NUTS and HMC"><meta name=twitter:description content="&ldquo;Bayesian modeling is harder than deep learning&rdquo; is a sentiment I&rsquo;ve been hearing a lot lately. While I&rsquo;m skeptical of sweeping statements like that, I agree when it comes to the central inference algorithm &mdash; how MCMC samplers work (especially the de facto standard samplers, NUTS and HMC) is one of the most difficult concepts I&rsquo;ve tried to learn, and is certainly harder than autodifferentiation or backpropagation.
So I thought I&rsquo;d share what worked for me when I tried to teach myself NUTS and HMC."><meta itemprop=name content="Understanding NUTS and HMC"><meta itemprop=description content="&ldquo;Bayesian modeling is harder than deep learning&rdquo; is a sentiment I&rsquo;ve been hearing a lot lately. While I&rsquo;m skeptical of sweeping statements like that, I agree when it comes to the central inference algorithm &mdash; how MCMC samplers work (especially the de facto standard samplers, NUTS and HMC) is one of the most difficult concepts I&rsquo;ve tried to learn, and is certainly harder than autodifferentiation or backpropagation.
So I thought I&rsquo;d share what worked for me when I tried to teach myself NUTS and HMC."><meta itemprop=datePublished content="2021-01-07T00:00:00+00:00"><meta itemprop=dateModified content="2021-01-07T00:00:00+00:00"><meta itemprop=wordCount content="308"><meta itemprop=image content="https://www.georgeho.org/assets/images/asterism.png"><meta itemprop=keywords content="bayes,"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({ tex2jax: {inlineMath: [["$", "$"]]} })
</script><script async src=//static.getclicky.com/101349618.js></script><noscript><p><img alt=Clicky width=1 height=1 src=//in.getclicky.com/101349618ns.gif></p></noscript><link rel=stylesheet media=all href=/assets/fonts/nicholson-gothic.css type=text/css><link rel=stylesheet media=all href=/assets/fonts/triplicate-a.css type=text/css><script type=text/javascript>navigator.appVersion.indexOf("Win")!=-1?document.write('<link rel="stylesheet" type="text/css" media="all" href="/assets/fonts/equity-a.css"/>'):navigator.appVersion.indexOf("Mac")!=-1?navigator.userAgent.match(/iPad/i)!=null?(document.write('<link rel="stylesheet" media="only screen and (max-device-width: 1024px)" href="/assets/fonts/equity-b.css" type="text/css"/>'),document.write('<link rel="stylesheet" media="only screen and (min-device-width: 768px) and (max-device-width: 1024px) and (-webkit-min-device-pixel-ratio: 2)" type="text/css" href="/assets/fonts/equity-a.css"/>')):document.write('<link rel="stylesheet" type="text/css" media="all" href="/assets/fonts/equity-b.css"/>'):document.write('<link rel="stylesheet" type="text/css" media="all" href="/assets/fonts/equity-a.css"/>')</script><style>html{font-size:18px;font-size:min(max(18px,4vw),26px)}content{line-height:1.45}body{max-width:800px;font-family:Equity,times new roman,Palatino,serif;background-color:#fffff8}code{font-family:Triplicate,lucida console,monospace;font-size:85%;background-color:unset}pre code{white-space:pre;overflow-x:auto;font-size:14px;font-size:min(max(12px,2vw),16px);text-size-adjust:100%;-ms-text-size-adjust:100%;-moz-text-size-adjust:100%;-webkit-text-size-adjust:100%}h1,h2,h3,h4,h5,h6{font-family:NicholsonGothic,Verdana,sans-serif;line-height:1.25}h2,h3,h4,h5,h6{margin-top:8%;margin-bottom:-1%}nav{margin-top:3%;margin-bottom:5%}p.wide-embeds{position:relative;left:50%;transform:translate(-50%,0);width:90vw}ul.blog-posts li span{flex:0 0 140px}.revue-form-group,.revue-form-actions{display:inline-block}input[type=email]{font-family:Triplicate,lucida console,monospace;font-size:80%}input[type=submit]{font-family:Equity,times new roman,Palatino,serif;font-size:80%}@media(prefers-color-scheme:dark){body{background-color:#111}}</style></head><body><header><a href=/ class=title><h2>⁂ George Ho</h2></a><nav><a href=/>Home</a>
<a href=/blog>Blog</a>
<a href=/crosswords/>Crosswords</a>
<a href=/work/>Work</a></nav></header><main><h1>Understanding NUTS and HMC</h1><p><i><time datetime=2021-01-07 pubdate>2021-01-07</time></i></p><content><p><em>&ldquo;Bayesian modeling is harder than deep learning&rdquo;</em> is a sentiment I&rsquo;ve been
hearing a lot lately. While I&rsquo;m skeptical of sweeping statements like that, I
agree when it comes to the central inference algorithm &mdash; how MCMC samplers
work (especially the <em>de facto</em> standard samplers, NUTS and HMC) is one of the
most difficult concepts I&rsquo;ve tried to learn, and is certainly harder than
autodifferentiation or backpropagation.</p><p>So I thought I&rsquo;d share what worked for me when I tried to teach myself NUTS and
HMC. In chronological order of publication, these are the three resources that
I’d recommend reading to grok NUTS/HMC:</p><ol><li><a href=http://www.mcmchandbook.net/HandbookChapter5.pdf>Radford Neal&rsquo;s chapter in the MCMC
handbook</a></li><li><a href=https://arxiv.org/abs/1111.4246>Matthew Hoffman’s <em>The No-U-Turn Sampler</em> (a.k.a. the original NUTS
paper)</a></li><li><a href=https://arxiv.org/abs/1701.02434>Michael Betancourt’s <em>Conceptual Introduction to Hamiltonian Monte
Carlo</em></a></li></ol><p>Not only did I find it useful to read these papers several times (as one would
read any sequence of &ldquo;important&rdquo; papers), but also to read them in both
chronological and reverse-chronological order. Reading both forwards and
backwards gave me multiple expositions of important ideas and also let me
mentally &ldquo;diff&rdquo; the papers to see the progression of ideas over time. For
example, Neal&rsquo;s chapter was written before NUTS was discovered, which gives you
a sense of what the MCMC world looked like prior to Hoffman&rsquo;s work: making
progress in fits and starts, but in need of a real leap forward.</p><p>In terms of reading code, I&rsquo;d recommend looking through <a href=https://github.com/ColCarroll/minimc>Colin Carroll’s
<code>minimc</code></a> for a minimal working example
of NUTS in Python, written for pedagogy rather than actual sampling. For a
&ldquo;real world&rdquo; implementation of NUTS/HMC, I’d recommend looking through <a href=https://github.com/eigenfoo/littlemcmc>my
<code>littlemcmc</code></a> for a standalone version
of PyMC3’s NUTS/HMC samplers.</p><p>Finally, for anyone who wants to read around computational methods for Bayesian
inference more generally (i.e. not restricted to HMC, for example), I&rsquo;d
(unashamedly) point to <a href=https://www.georgeho.org/bayesian-inference-reading/>my blog post on
this</a>.</p></content><p><a href=https://www.georgeho.org/blog/bayes/>#bayes</a></p></main><footer><form action=https://buttondown.email/api/emails/embed-subscribe/eigenfoo method=post target=popupwindow onsubmit='window.open("https://buttondown.email/eigenfoo","popupwindow")' class=embeddable-buttondown-form><label for=bd-email>Get my (monthly?) blog posts over email:</label><br><input type=email name=email id=bd-email placeholder=your@email.com>
<input type=submit value=Subscribe></form></footer></body></html>