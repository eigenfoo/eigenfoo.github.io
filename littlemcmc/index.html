<!doctype html><html lang=en-us><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://www.georgeho.org/favicon.ico><title>`littlemcmc` — A Standalone HMC and NUTS Sampler in Python | George Ho</title><meta name=title content="`littlemcmc` — A Standalone HMC and NUTS Sampler in Python"><meta name=description content="Recently there has been a modularization (or, if you&rsquo;re hip with tech-lingo, an unbundling) of Bayesian modelling libraries. Whereas before, probability distributions, model specification, inference and diagnostics were more or less rolled into one library, it&rsquo;s becoming more and more realistic to specify a model in one library, accelerate it using another, perform inference with a third and use a fourth to visualize the results. (For example, Junpeng Lao has recently had good success doing exactly this!"><meta name=keywords content="bayes,open-source,probabilistic-programming,pymc,python,"><meta property="og:title" content="`littlemcmc` — A Standalone HMC and NUTS Sampler in Python"><meta property="og:description" content="Recently there has been a modularization (or, if you&rsquo;re hip with tech-lingo, an unbundling) of Bayesian modelling libraries. Whereas before, probability distributions, model specification, inference and diagnostics were more or less rolled into one library, it&rsquo;s becoming more and more realistic to specify a model in one library, accelerate it using another, perform inference with a third and use a fourth to visualize the results. (For example, Junpeng Lao has recently had good success doing exactly this!"><meta property="og:type" content="article"><meta property="og:url" content="https://www.georgeho.org/littlemcmc/"><meta property="og:image" content="https://www.georgeho.org/assets/images/asterism.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2020-10-06T00:00:00+00:00"><meta property="article:modified_time" content="2020-10-06T00:00:00+00:00"><meta property="og:site_name" content="⁂ George Ho"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.georgeho.org/assets/images/asterism.png"><meta name=twitter:title content="`littlemcmc` — A Standalone HMC and NUTS Sampler in Python"><meta name=twitter:description content="Recently there has been a modularization (or, if you&rsquo;re hip with tech-lingo, an unbundling) of Bayesian modelling libraries. Whereas before, probability distributions, model specification, inference and diagnostics were more or less rolled into one library, it&rsquo;s becoming more and more realistic to specify a model in one library, accelerate it using another, perform inference with a third and use a fourth to visualize the results. (For example, Junpeng Lao has recently had good success doing exactly this!"><meta itemprop=name content="`littlemcmc` — A Standalone HMC and NUTS Sampler in Python"><meta itemprop=description content="Recently there has been a modularization (or, if you&rsquo;re hip with tech-lingo, an unbundling) of Bayesian modelling libraries. Whereas before, probability distributions, model specification, inference and diagnostics were more or less rolled into one library, it&rsquo;s becoming more and more realistic to specify a model in one library, accelerate it using another, perform inference with a third and use a fourth to visualize the results. (For example, Junpeng Lao has recently had good success doing exactly this!"><meta itemprop=datePublished content="2020-10-06T00:00:00+00:00"><meta itemprop=dateModified content="2020-10-06T00:00:00+00:00"><meta itemprop=wordCount content="227"><meta itemprop=image content="https://www.georgeho.org/assets/images/asterism.png"><meta itemprop=keywords content="bayes,open-source,probabilistic-programming,pymc,python,"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({ tex2jax: {inlineMath: [["$", "$"]]} })
</script><script async src=//static.getclicky.com/101349618.js></script><noscript><p><img alt=Clicky width=1 height=1 src=//in.getclicky.com/101349618ns.gif></p></noscript><link rel=stylesheet media=all href=/assets/fonts/nicholson-gothic.css type=text/css><link rel=stylesheet media=all href=/assets/fonts/triplicate-a.css type=text/css><script type=text/javascript>navigator.appVersion.indexOf("Win")!=-1?document.write('<link rel="stylesheet" type="text/css" media="all" href="/assets/fonts/equity-a.css"/>'):navigator.appVersion.indexOf("Mac")!=-1?navigator.userAgent.match(/iPad/i)!=null?(document.write('<link rel="stylesheet" media="only screen and (max-device-width: 1024px)" href="/assets/fonts/equity-b.css" type="text/css"/>'),document.write('<link rel="stylesheet" media="only screen and (min-device-width: 768px) and (max-device-width: 1024px) and (-webkit-min-device-pixel-ratio: 2)" type="text/css" href="/assets/fonts/equity-a.css"/>')):document.write('<link rel="stylesheet" type="text/css" media="all" href="/assets/fonts/equity-b.css"/>'):document.write('<link rel="stylesheet" type="text/css" media="all" href="/assets/fonts/equity-a.css"/>')</script><style>html{font-size:18px;font-size:min(max(1rem,4vw),22px)}body{font-family:Equity,Georgia,serif;background-color:#fffff8}code{font-family:Triplicate,lucida console,monospace}h1,h2,h3,h4,h5,h6{font-family:NicholsonGothic,Verdana,sans-serif}@media(prefers-color-scheme:dark){body{background-color:#111}}</style></head><body><header><a href=/ class=title><h2>⁂ George Ho</h2></a><nav><a href=/>Home</a>
<a href=/work/>Work</a>
<a href=/blog>Blog</a></nav></header><main><h1><code>littlemcmc</code> — A Standalone HMC and NUTS Sampler in Python</h1><p><i><time datetime=2020-10-06 pubdate>2020-10-06</time></i></p><content><center><img src=https://raw.githubusercontent.com/eigenfoo/littlemcmc/master/docs/_static/logo/default-cropped.png alt="LittleMCMC logo"></center><p>Recently there has been a modularization (or, if you&rsquo;re hip with tech-lingo, an
<a href=https://techcrunch.com/2015/04/18/the-unbundling-of-everything/><em>unbundling</em></a>)
of Bayesian modelling libraries. Whereas before, probability distributions,
model specification, inference and diagnostics were more or less rolled into one
library, it&rsquo;s becoming more and more realistic to specify a model in one
library, accelerate it using another, perform inference with a third and use a
fourth to visualize the results. (For example, Junpeng Lao has recently had
<a href=https://twitter.com/junpenglao/status/1309470970223226882>good success</a> doing
exactly this!)</p><p>It&rsquo;s in this spirit of unbundling that the PyMC developers wanted to <a href=https://discourse.pymc.io/t/isolate-nuts-into-a-new-library/3974>spin out
the core HMC and NUTS samplers from PyMC3 into a separate
library</a>.
PyMC3 has a very well-tested and performant Python implementation of HMC and
NUTS, which would be very useful to any users who have their own functions for
computing log-probability and its gradients, and who want to use a lightweight
and reliable sampler.</p><p>So for example, if you&rsquo;re a physical scientist with a Bayesian model who&rsquo;s
written your own functions to compute the log probability and its gradients
(perhaps for performance or interoperability reasons), and need a good MCMC
sampler, then <code>littlemcmc</code> is for you! As long as you can call your functions
from Python, you can use the same HMC or NUTS sampler that&rsquo;s used by the rest of
the PyMC3 community.</p><p>So without further ado: please check out <code>littlemcmc</code>!</p><ul><li><a href=https://github.com/eigenfoo/littlemcmc>GitHub</a></li><li><a href=https://littlemcmc.readthedocs.io/en/latest/>Read the Docs</a></li></ul></content><p><a href=https://www.georgeho.org/blog/bayes/>#bayes</a>
<a href=https://www.georgeho.org/blog/open-source/>#open-source</a>
<a href=https://www.georgeho.org/blog/probabilistic-programming/>#probabilistic-programming</a>
<a href=https://www.georgeho.org/blog/pymc/>#pymc</a>
<a href=https://www.georgeho.org/blog/python/>#python</a></p></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>