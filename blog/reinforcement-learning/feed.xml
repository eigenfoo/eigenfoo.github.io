<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>reinforcement-learning on George Ho</title><link>https://www.georgeho.org/blog/reinforcement-learning/</link><description>Recent content in reinforcement-learning on George Ho</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><copyright>Copyright © 2022, George Ho.</copyright><lastBuildDate>Sun, 02 Jun 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://www.georgeho.org/blog/reinforcement-learning/feed.xml" rel="self" type="application/rss+xml"/><item><title>Decaying Evidence and Contextual Bandits — Bayesian Reinforcement Learning (Part 2)</title><link>https://www.georgeho.org/bayesian-bandits-2/</link><pubDate>Sun, 02 Jun 2019 00:00:00 +0000</pubDate><guid>https://www.georgeho.org/bayesian-bandits-2/</guid><description>&lt;blockquote>
&lt;p>This is the second of a two-part series about Bayesian bandit algorithms.
Check out the first post &lt;a href="https://www.georgeho.org/bayesian-bandits/">here&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://www.georgeho.org/bayesian-bandits/">Previously&lt;/a>, I introduced the
multi-armed bandit problem, and a Bayesian approach to solving/modelling it
(Thompson sampling). We saw that conjugate models made it possible to run the
bandit algorithm online: the same is even true for non-conjugate models, so long
as the rewards are bounded.&lt;/p>
&lt;p>In this follow-up blog post, we&amp;rsquo;ll take a look at two extensions to the
multi-armed bandit. The first allows the bandit to model nonstationary rewards
distributions, whereas the second allows the bandit to model context. Jump in!&lt;/p>
&lt;figure>
&lt;a href="https://www.georgeho.org/assets/images/multi-armed-bandit.jpg">&lt;img src="https://www.georgeho.org/assets/images/multi-armed-bandit.jpg" alt="Cartoon of a multi-armed bandit">&lt;/a>
&lt;figcaption>An example of a multi-armed bandit situation. Source: &lt;a href="https://www.inverse.com/article/13762-how-the-multi-armed-bandit-determines-what-ads-and-stories-you-see-online">Inverse&lt;/a>.&lt;/figcaption>
&lt;/figure>
&lt;h2 id="nonstationary-bandits">Nonstationary Bandits&lt;/h2>
&lt;p>Up until now, we&amp;rsquo;ve concerned ourselves with stationary bandits: in other words,
we assumed that the rewards distribution for each arm did not change over time.
In the real world though, rewards distributions need not be stationary: customer
preferences change, trading algorithms deteriorate, and news articles rise and
fall in relevance.&lt;/p>
&lt;p>Nonstationarity could mean one of two things for us:&lt;/p>
&lt;ol>
&lt;li>either we are lucky enough to know that rewards are similarly distributed
throughout all time (e.g. the rewards are always normally distributed, or
always binomially distributed), and that it is merely the parameters of these
distributions that are liable to change,&lt;/li>
&lt;li>or we aren&amp;rsquo;t so unlucky, and the rewards distributions are not only changing,
but don&amp;rsquo;t even have a nice parametric form.&lt;/li>
&lt;/ol>
&lt;p>Good news, though: there is a neat trick to deal with both forms of
nonstationarity!&lt;/p>
&lt;h3 id="decaying-evidence-and-posteriors">Decaying evidence and posteriors&lt;/h3>
&lt;p>But first, some notation. Suppose we have a model with parameters $\theta$. We
place a prior $\color{purple}{\pi_0(\theta)}$ on it&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, and at the $t$&amp;lsquo;th
time step, we observe data $D_t$, compute the likelihood $\color{blue}{P(D_t
| \theta)}$ and update the posterior from $\color{red}{\pi_t(\theta |
D_{1:t})}$ to $\color{green}{\pi_{t+1}(\theta | D_{1:t+1})}$.&lt;/p>
&lt;p>This is a quintessential application of Bayes&amp;rsquo; Theorem. Mathematically:&lt;/p>
&lt;p>$$ \color{green}{\pi_{t+1}(\theta | D_{1:t+1})} \propto \color{blue}{P(D_{t+1} |
\theta)} \cdot \color{red}{\pi_t (\theta | D_{1:t})} \tag{1} \label{1} $$&lt;/p>
&lt;p>However, for problems with nonstationary rewards distributions, we would like
data points observed a long time ago to have less weight than data points
observed recently. This is only prudent: in the absence of recent data, we would
like to adopt a more conservative &amp;ldquo;no-data&amp;rdquo; prior, rather than allow our
posterior to be informed by outdated data. This can be achieved by modifying the
Bayesian update to:&lt;/p>
&lt;p>$$ \color{green}{\pi_{t+1}(\theta | D_{1:t+1})} \propto \color{magenta}{[}
\color{blue}{P(D_{t+1} | \theta)} \cdot \color{red}{\pi_t (\theta | D_{1:t})}
{\color{magenta}{]^{1-\epsilon}}} \cdot
\color{purple}{\pi_0(\theta)}^\color{magenta}{\epsilon} \tag{2} \label{2} $$&lt;/p>
&lt;p>for some $0 &amp;lt; \color{magenta}{\epsilon} \ll 1$. We can think of
$\color{magenta}{\epsilon}$ as controlling the rate of decay of the
evidence/posterior (i.e. how quickly we should distrust past data points).
Notice that if we stop observing data points at time $T$, then
$\color{red}{\pi_t(\theta | D_{1:T})} \rightarrow
\color{purple}{\pi_0(\theta)}$ as $t \rightarrow \infty$.&lt;/p>
&lt;p>Decaying the evidence (and therefore the posterior) can be used to address both
types of nonstationarity identified above. Simply use $(\ref{2})$ as a drop-in
replacement for $(\ref{1})$ when updating the hyperparameters. Whether you&amp;rsquo;re
using a conjugate model or the algorithm by &lt;a href="https://arxiv.org/abs/1111.1797">Agarwal and
Goyal&lt;/a> (introduced in &lt;a href="https://www.georgeho.org/bayesian-bandits">the previous blog
post&lt;/a>), using $(\ref{2})$ will decay
the evidence and posterior, as desired.&lt;/p>
&lt;p>For more information (and a worked example for the Beta-Binomial model!), check
out &lt;a href="https://austinrochford.com/resources/talks/boston-bayesians-2017-bayes-bandits.slides.html#/3">Austin Rochford&amp;rsquo;s talk for Boston
Bayesians&lt;/a>
about Bayesian bandit algorithms for e-commerce.&lt;/p>
&lt;h2 id="contextual-bandits">Contextual Bandits&lt;/h2>
&lt;p>We can think of the multi-armed bandit problem as follows&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>:&lt;/p>
&lt;ol>
&lt;li>A policy chooses an arm $a$ from $k$ arms.&lt;/li>
&lt;li>The world reveals the reward $R_a$ of the chosen arm.&lt;/li>
&lt;/ol>
&lt;p>However, this formulation fails to capture an important phenomenon: there is
almost always extra information that is available when making each decision.
For instance, online ads occur in the context of the web page in which they
appear, and online store recommendations are given in the context of the user&amp;rsquo;s
current cart contents (among other things).&lt;/p>
&lt;p>To take advantage of this information, we might think of a different formulation
where, on each round:&lt;/p>
&lt;ol>
&lt;li>The world announces some context information $x$.&lt;/li>
&lt;li>A policy chooses an arm $a$ from $k$ arms.&lt;/li>
&lt;li>The world reveals the reward $R_a$ of the chosen arm.&lt;/li>
&lt;/ol>
&lt;p>In other words, contextual bandits call for some way of taking context as input
and producing arms/actions as output.&lt;/p>
&lt;p>Alternatively, if you think of regular multi-armed bandits as taking no input
whatsoever (but still producing outputs, the arms to pull), you can think of
contextual bandits as algorithms that both take inputs and produce outputs.&lt;/p>
&lt;h3 id="bayesian-contextual-bandits">Bayesian contextual bandits&lt;/h3>
&lt;p>Contextual bandits give us a very general framework for thinking about
sequential decision making (and reinforcement learning). Clearly, there are many
ways to make a bandit algorithm take context into account. Linear regression is
a straightforward and classic example: simply assume that the rewards depend
linearly on the context.&lt;/p>
&lt;p>For a refresher on the details of Bayesian linear regression, refer to &lt;a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book">&lt;em>Pattern
Recognition and Machine
Learning&lt;/em>&lt;/a>
by Christopher Bishop: specifically, section 3.3 on Bayesian linear regression
and exercises 3.12 and 3.13&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Briefly though, if we place a Gaussian prior on
the regression weights and an inverse gamma prior on the noise parameter (i.e.,
the noise of the observations), then their joint prior will be conjugate to a
Gaussian likelihood, and the posterior predictive distribution for the rewards
will be a Student&amp;rsquo;s $t$.&lt;/p>
&lt;p>Since we need to maintain posteriors of the rewards for each arm (so that we can
do Thompson sampling), we need to run a separate Bayesian linear regression for
each arm. At every iteration we then Thompson sample from each Student&amp;rsquo;s $t$
posterior, and select the arm with the highest sample.&lt;/p>
&lt;p>However, Bayesian linear regression is a textbook example of a model that lacks
expressiveness: in most circumstances, we want something that can model
nonlinear functions as well. One (perfectly valid) way of doing this would be to
hand-engineer some nonlinear features and/or basis functions before feeding them
into a Bayesian linear regression. However, in the 21st century, the trendier
thing to do is to have a neural network learn those features for you. This is
exactly what is proposed in a &lt;a href="https://arxiv.org/abs/1802.09127">ICLR 2018 paper from Google
Brain&lt;/a>. They find that this model — which they
call &lt;code>NeuralLinear&lt;/code> — performs decently well across a variety of tasks, even
compared to other bandit algorithms. In the words of the authors:&lt;/p>
&lt;blockquote>
&lt;p>We believe [&lt;code>NeuralLinear&lt;/code>&amp;rsquo;s] main strength is that it is able to
&lt;em>simultaneously&lt;/em> learn a data representation that greatly simplifies the task
at hand, and to accurately quantify the uncertainty over linear models that
explain the observed rewards in terms of the proposed representation.&lt;/p>
&lt;/blockquote>
&lt;p>For more information, be sure to check out the &lt;a href="https://arxiv.org/abs/1802.09127">Google Brain
paper&lt;/a> and the accompanying &lt;a href="https://github.com/tensorflow/models/tree/master/research/deep_contextual_bandits">TensorFlow
code&lt;/a>.&lt;/p>
&lt;h2 id="further-reading">Further Reading&lt;/h2>
&lt;p>For non-Bayesian approaches to contextual bandits, &lt;a href="https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Contextual-Bandit-algorithms">Vowpal
Wabbit&lt;/a>
is a great resource: &lt;a href="http://hunch.net/~jl/">John Langford&lt;/a> and the team at
&lt;a href="https://www.microsoft.com/research/">Microsoft Research&lt;/a> has &lt;a href="https://arxiv.org/abs/1402.0555v2">extensively
researched&lt;/a> contextual bandit algorithms.
They&amp;rsquo;ve provided blazingly fast implementations of recent algorithms and written
good documentation for them.&lt;/p>
&lt;p>For the theory and math behind bandit algorithms, &lt;a href="https://banditalgs.com/">Tor Lattimore and Csaba
Szepesvári&amp;rsquo;s book&lt;/a> covers a breathtaking amount of
ground.&lt;/p>
&lt;blockquote>
&lt;p>This is the second of a two-part series about Bayesian bandit algorithms.
Check out the first post &lt;a href="https://www.georgeho.org/bayesian-bandits/">here&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Did you know you can make &lt;a href="http://adereth.github.io/blog/2013/11/29/colorful-equations/">colored equations with
MathJax&lt;/a>?
Technology frightens me sometimes.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>This explanation is largely drawn from &lt;a href="http://hunch.net/?p=298">from John Langford&amp;rsquo;s
&lt;code>hunch.net&lt;/code>&lt;/a>.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>If you don&amp;rsquo;t want to do Bishop&amp;rsquo;s exercises, there&amp;rsquo;s a partially complete
solutions manual &lt;a href="https://github.com/GoldenCheese/PRML-Solution-Manual/">on
GitHub&lt;/a> 😉&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Multi-Armed Bandits and Conjugate Models — Bayesian Reinforcement Learning (Part 1)</title><link>https://www.georgeho.org/bayesian-bandits/</link><pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate><guid>https://www.georgeho.org/bayesian-bandits/</guid><description>&lt;blockquote>
&lt;p>This is the first of a two-part series about Bayesian bandit algorithms. Check
out the second post &lt;a href="https://www.georgeho.org/bayesian-bandits-2/">here&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>Let&amp;rsquo;s talk about Bayesianism. It&amp;rsquo;s developed a reputation (not entirely
justified, but not entirely unjustified either) for being too mathematically
sophisticated or too computationally intensive to work at scale. For instance,
inferring from a Gaussian mixture model is fraught with computational problems
(hierarchical funnels, multimodal posteriors, etc.), and may take a seasoned
Bayesian anywhere between a day and a month to do well. On the other hand, other
blunt hammers of estimation are as easy as a maximum likelihood estimate:
something you could easily get a SQL query to do if you wanted to.&lt;/p>
&lt;p>In this blog post I hope to show that there is more to Bayesianism than just
MCMC sampling and suffering, by demonstrating a Bayesian approach to a classic
reinforcement learning problem: the &lt;em>multi-armed bandit&lt;/em>.&lt;/p>
&lt;p>The problem is this: imagine a gambler at a row of slot machines (each machine
being a “one-armed bandit”), who must devise a strategy so as to maximize
rewards. This strategy includes which machines to play, how many times to play
each machine, in which order to play them, and whether to continue with the
current machine or try a different machine.&lt;/p>
&lt;p>This problem is a central problem in decision theory and reinforcement learning:
the agent (our gambler) starts out in a state of ignorance, but learns through
interacting with its environment (playing slots). For more details, Cam
Davidson-Pilon has a great introduction to multi-armed bandits in Chapter 6 of
his book &lt;a href="https://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter6_Priorities/Ch6_Priors_PyMC3.ipynb">&lt;em>Bayesian Methods for
Hackers&lt;/em>&lt;/a>,
and Tor Lattimore and Csaba Szepesvári cover a breathtaking amount of the
underlying theory in their book &lt;a href="http://banditalgs.com/">&lt;em>Bandit Algorithms&lt;/em>&lt;/a>.&lt;/p>
&lt;p>So let&amp;rsquo;s get started! I assume that you are familiar with:&lt;/p>
&lt;ul>
&lt;li>some basic probability, at least enough to know some distributions: normal,
Bernoulli, binomial&amp;hellip;&lt;/li>
&lt;li>some basic Bayesian statistics, at least enough to understand what a
&lt;a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate prior&lt;/a> (and
conjugate model) is, and why one might like them.&lt;/li>
&lt;li>&lt;a href="https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/">Python generators and the &lt;code>yield&lt;/code>
keyword&lt;/a>,
to understand some of the code I&amp;rsquo;ve written&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/li>
&lt;/ul>
&lt;p>Dive in!&lt;/p>
&lt;h2 id="the-algorithm">The Algorithm&lt;/h2>
&lt;p>The algorithm is straightforward. The description below is taken from Cam
Davidson-Pilon over at Data Origami&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>For each round,&lt;/p>
&lt;ol>
&lt;li>Sample a random variable $X_b$ from the prior of bandit $b$, for all
$b$.&lt;/li>
&lt;li>Select the bandit with largest sample, i.e. select bandit $B =
\text{argmax}(X_b)$.&lt;/li>
&lt;li>Observe the result of pulling bandit $B$, and update your prior on bandit
$B$ using the conjugate model update rule.&lt;/li>
&lt;li>Repeat!&lt;/li>
&lt;/ol>
&lt;p>What I find remarkable about this is how dumbfoundingly simple it is! No MCMC
sampling, no $\hat{R}$s to diagnose, no pesky divergences&amp;hellip; all it requires is
a conjugate model, and the rest is literally just counting.&lt;/p>
&lt;p>&lt;strong>NB:&lt;/strong> This algorithm is technically known as &lt;em>Thompson sampling&lt;/em>, and is only
one of many algorithms out there. The main difference is that there are other
ways to go from our current priors to a decision on which bandit to play
next. E.g. instead of simply sampling from our priors, we could use the
upper bound of the 90% credible region, or some dynamic quantile of the
posterior (as in Bayes UCB). See Data Origami&lt;sup id="fnref1:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> for more information.&lt;/p>
&lt;h3 id="stochastic-aka-stationary-bandits">Stochastic (a.k.a. stationary) bandits&lt;/h3>
&lt;p>Let&amp;rsquo;s take this algorithm for a spin! Assume we have rewards which are Bernoulli
distributed (this would be the situation we face when e.g. modelling
click-through rates). The conjugate prior for the Bernoulli distribution is the
Beta distribution (this is a special case of the Beta-Binomial model).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">make_bandits&lt;/span>(params):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">pull&lt;/span>(arm, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">None&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Bernoulli distributed rewards&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reward &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>random&lt;span style="color:#f92672">.&lt;/span>binomial(n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, p&lt;span style="color:#f92672">=&lt;/span>params[arm], size&lt;span style="color:#f92672">=&lt;/span>size)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">yield&lt;/span> reward
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> pull, len(params)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">bayesian_strategy&lt;/span>(pull, num_bandits):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_rewards &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>zeros(num_bandits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_trials &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>zeros(num_bandits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Sample from the bandits&amp;#39; priors, and choose largest&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> choice &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>argmax(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> np&lt;span style="color:#f92672">.&lt;/span>random&lt;span style="color:#f92672">.&lt;/span>beta(a&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#f92672">+&lt;/span> num_rewards, b&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#f92672">+&lt;/span> num_trials &lt;span style="color:#f92672">-&lt;/span> num_rewards)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Sample the chosen bandit&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reward &lt;span style="color:#f92672">=&lt;/span> next(pull(choice))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Update&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_rewards[choice] &lt;span style="color:#f92672">+=&lt;/span> reward
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_trials[choice] &lt;span style="color:#f92672">+=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">yield&lt;/span> choice, reward, num_rewards, num_trials
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> __name__ &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#e6db74">&amp;#34;__main__&amp;#34;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pull, num_bandits &lt;span style="color:#f92672">=&lt;/span> make_bandits([&lt;span style="color:#ae81ff">0.2&lt;/span>, &lt;span style="color:#ae81ff">0.5&lt;/span>, &lt;span style="color:#ae81ff">0.7&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> play &lt;span style="color:#f92672">=&lt;/span> bayesian_strategy(pull, num_bandits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> _ &lt;span style="color:#f92672">in&lt;/span> range(&lt;span style="color:#ae81ff">100&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> choice, reward, num_rewards, num_trials &lt;span style="color:#f92672">=&lt;/span> next(play)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here, &lt;code>pull&lt;/code> returns the result of pulling on the &lt;code>arm&lt;/code>&amp;lsquo;th bandit, and
&lt;code>make_bandits&lt;/code> is just a factory function for &lt;code>pull&lt;/code>.&lt;/p>
&lt;p>The &lt;code>bayesian_strategy&lt;/code> function actually implements the algorithm. We only need
to keep track of the number of times we win and the number of times we played
(&lt;code>num_rewards&lt;/code> and &lt;code>num_trials&lt;/code>, respectively). It samples from all current
&lt;code>np.random.beta&lt;/code> priors (where the original prior was a $\text{Beta}(2,
2)$, which is symmetrix about 0.5 and explains the odd-looking &lt;code>a=2+&lt;/code> and
&lt;code>b=2+&lt;/code> there), picks the &lt;code>np.argmax&lt;/code>, &lt;code>pull&lt;/code>s that specific bandit, and updates
&lt;code>num_rewards&lt;/code> and &lt;code>num_trials&lt;/code>.&lt;/p>
&lt;p>I&amp;rsquo;ve omitted the data visualization code here, but if you want to see it, check
out the &lt;a href="https://github.com/eigenfoo/wanderings/blob/afcf37a8c6c2a2ac38f6708c1f3dd50db2ebe71f/bayes/bayesian-bandits.ipynb">Jupyter notebook on my
GitHub&lt;/a>&lt;/p>
&lt;figure>
&lt;a href="https://www.georgeho.org/assets/images/beta-binomial.png">&lt;img style="float: middle" src="https://www.georgeho.org/assets/images/beta-binomial.png" alt="Posterior distribution after several pulls for the Beta-Binomial model">&lt;/a>
&lt;/figure>
&lt;h3 id="generalizing-to-conjugate-models">Generalizing to conjugate models&lt;/h3>
&lt;p>In fact, this algorithm isn&amp;rsquo;t just limited to Bernoulli-distributed rewards: it
will work for any &lt;a href="https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions">conjugate
model&lt;/a>!
Here I implement the Gamma-Poisson model (that is, Poisson distributed rewards,
with a Gamma conjugate prior) to illustrate how extensible this framework is.
(Who cares about Poisson distributed rewards, you ask? Anyone who worries about
returning customers, for one!)&lt;/p>
&lt;p>Here&amp;rsquo;s what we need to change:&lt;/p>
&lt;ul>
&lt;li>The rewards distribution in the &lt;code>pull&lt;/code> function (in practice, you don&amp;rsquo;t get
to pick this, so &lt;em>technically&lt;/em> there&amp;rsquo;s nothing to change if you&amp;rsquo;re doing this
in production!)&lt;/li>
&lt;li>The sampling from the prior in &lt;code>bayesian_strategy&lt;/code>&lt;/li>
&lt;li>The variables you need to keep track of and the update rule in &lt;code>bayesian_strategy&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Without further ado:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">make_bandits&lt;/span>(params):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">pull&lt;/span>(arm, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">None&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Poisson distributed rewards&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reward &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>random&lt;span style="color:#f92672">.&lt;/span>poisson(lam&lt;span style="color:#f92672">=&lt;/span>params[arm], size&lt;span style="color:#f92672">=&lt;/span>size)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">yield&lt;/span> reward
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> pull, len(params)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">bayesian_strategy&lt;/span>(pull, num_bandits):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_rewards &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>ones(num_bandits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_trials &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>ones(num_bandits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Sample from the bandits&amp;#39; priors, and choose largest&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> choice &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>argmax(np&lt;span style="color:#f92672">.&lt;/span>random&lt;span style="color:#f92672">.&lt;/span>gamma(num_rewards, scale&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">/&lt;/span> num_trials))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Sample the chosen bandit&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reward &lt;span style="color:#f92672">=&lt;/span> next(pull(choice))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Update&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_rewards[choice] &lt;span style="color:#f92672">+=&lt;/span> reward
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_trials[choice] &lt;span style="color:#f92672">+=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">yield&lt;/span> choice, reward, num_rewards, num_trials
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> __name__ &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#e6db74">&amp;#34;__main__&amp;#34;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pull, num_bandits &lt;span style="color:#f92672">=&lt;/span> make_bandits([&lt;span style="color:#ae81ff">4.0&lt;/span>, &lt;span style="color:#ae81ff">4.5&lt;/span>, &lt;span style="color:#ae81ff">5.0&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> play &lt;span style="color:#f92672">=&lt;/span> bayesian_strategy(pull, num_bandits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> _ &lt;span style="color:#f92672">in&lt;/span> range(&lt;span style="color:#ae81ff">100&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> choice, reward, num_rewards, num_trials &lt;span style="color:#f92672">=&lt;/span> next(play)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;figure>
&lt;a href="https://www.georgeho.org/assets/images/gamma-poisson.png">&lt;img style="float: middle" src="https://www.georgeho.org/assets/images/gamma-poisson.png" alt="Posterior distribution after several pulls for the Gamma-Poisson model">&lt;/a>
&lt;/figure>
&lt;p>This really demonstrates how lean and mean conjugate models can be, especially
considering how much of a pain MCMC or approximate inference methods would be,
compared to literal &lt;em>counting&lt;/em>. Conjugate models aren&amp;rsquo;t just textbook examples:
they&amp;rsquo;re &lt;em>(gasp)&lt;/em> actually useful!&lt;/p>
&lt;h3 id="generalizing-to-arbitrary-rewards-distributions">Generalizing to arbitrary rewards distributions&lt;/h3>
&lt;p>OK, so if we have a conjugate model, we can use Thompson sampling to solve the
multi-armed bandit problem. But what if our rewards distribution doesn&amp;rsquo;t have a
conjugate prior, or what if we don&amp;rsquo;t even &lt;em>know&lt;/em> our rewards distribution?&lt;/p>
&lt;p>In general this problem is very difficult to solve. Theoretically, we could
place some fairly uninformative prior on our rewards, and after every pull we
could run MCMC to get our posterior, but that doesn&amp;rsquo;t scale, especially for the
online algorithms that we have in mind. Luckily a recent paper by Agrawal and
Goyal&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> gives us some help, &lt;em>if we assume rewards are bounded on the interval
$[0, 1]$&lt;/em> (of course, if we have bounded rewards, then we can just normalize
them by their maximum value to get rewards between 0 and 1).&lt;/p>
&lt;p>This solutions bootstraps the first Beta-Bernoulli model to this new situation.
Here&amp;rsquo;s what happens:&lt;/p>
&lt;ol>
&lt;li>Sample a random variable $X_b$ from the (Beta) prior of bandit $b$, for
all $b$.&lt;/li>
&lt;li>Select the bandit with largest sample, i.e. select bandit $B =
\text{argmax}(X_b)$.&lt;/li>
&lt;li>Observe the reward $R$ from bandit $B$.&lt;/li>
&lt;li>&lt;strong>Observe the outcome $r$ from a Bernoulli trial with probability of success $R$.&lt;/strong>&lt;/li>
&lt;li>Update posterior of $B$ with this observation $r$.&lt;/li>
&lt;li>Repeat!&lt;/li>
&lt;/ol>
&lt;p>Here I do this for the logit-normal distribution (i.e. a random variable whose
logit is normally distributed). Note that &lt;code>np.expit&lt;/code> is the inverse of the logit
function.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">make_bandits&lt;/span>(params):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">pull&lt;/span>(arm, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">None&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Logit-normal distributed returns (or any distribution with finite support)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># `expit` is the inverse of `logit`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reward &lt;span style="color:#f92672">=&lt;/span> expit(np&lt;span style="color:#f92672">.&lt;/span>random&lt;span style="color:#f92672">.&lt;/span>normal(loc&lt;span style="color:#f92672">=&lt;/span>params[arm], scale&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, size&lt;span style="color:#f92672">=&lt;/span>size))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">yield&lt;/span> reward
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> pull, len(params)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">bayesian_strategy&lt;/span>(pull, num_bandits):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_rewards &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>zeros(num_bandits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_trials &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>zeros(num_bandits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Sample from the bandits&amp;#39; priors, and choose largest&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> choice &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>argmax(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> np&lt;span style="color:#f92672">.&lt;/span>random&lt;span style="color:#f92672">.&lt;/span>beta(&lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#f92672">+&lt;/span> num_rewards, &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#f92672">+&lt;/span> num_trials &lt;span style="color:#f92672">-&lt;/span> num_rewards)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Sample the chosen bandit&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reward &lt;span style="color:#f92672">=&lt;/span> next(pull(choice))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Sample a Bernoulli with probability of success = reward&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Remember, reward is normalized to be in [0, 1]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> outcome &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>random&lt;span style="color:#f92672">.&lt;/span>binomial(n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, p&lt;span style="color:#f92672">=&lt;/span>reward)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Update&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_rewards[choice] &lt;span style="color:#f92672">+=&lt;/span> outcome
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_trials[choice] &lt;span style="color:#f92672">+=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">yield&lt;/span> choice, reward, num_rewards, num_trials
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> __name__ &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#e6db74">&amp;#34;__main__&amp;#34;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pull, num_bandits &lt;span style="color:#f92672">=&lt;/span> make_bandits([&lt;span style="color:#ae81ff">0.2&lt;/span>, &lt;span style="color:#ae81ff">1.8&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> play &lt;span style="color:#f92672">=&lt;/span> bayesian_strategy(pull, num_bandits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> _ &lt;span style="color:#f92672">in&lt;/span> range(&lt;span style="color:#ae81ff">100&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> choice, reward, num_rewards, num_trials &lt;span style="color:#f92672">=&lt;/span> next(play)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;figure>
&lt;a href="https://www.georgeho.org/assets/images/bounded.png">&lt;img style="float: middle" src="https://www.georgeho.org/assets/images/bounded.png" alt="Posterior distribution after several pulls with an arbitrary reward distribution (e.g. the logit normal)">&lt;/a>
&lt;/figure>
&lt;h2 id="final-remarks">Final Remarks&lt;/h2>
&lt;p>None of this theory is new: I&amp;rsquo;m just advertising it! See Cam Davidson-Pilon&amp;rsquo;s
great blog post about Bayesian bandits&lt;sup id="fnref2:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> for a much more in-depth treatment,
and of course, read around papers on arXiv if you want to go deeper!&lt;/p>
&lt;p>Also, if you want to see all the code that went into this blog post, check out
&lt;a href="https://github.com/eigenfoo/wanderings/blob/afcf37a8c6c2a2ac38f6708c1f3dd50db2ebe71f/bayes/bayesian-bandits.ipynb">the notebook
here&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>This is the first of a two-part series about Bayesian bandit algorithms. Check
out the second post &lt;a href="https://www.georgeho.org/bayesian-bandits-2/">here&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>I&amp;rsquo;ve hopped on board the functional programming bandwagon, and couldn&amp;rsquo;t
help but think that to demonstrate this idea, I didn&amp;rsquo;t need a framework, a
library or even a class. Just two functions!&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Davidson-Pilon, Cameron. “Multi-Armed Bandits.” DataOrigami, 6 Apr. 2013,
&lt;a href="https://dataorigami.net/blogs/napkin-folding/79031811-multi-armed-bandits">dataorigami.net/blogs/napkin-folding/79031811-multi-armed-bandits&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref2:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a href="https://arxiv.org/abs/1111.1797">arXiv:1111.1797&lt;/a> [cs.LG]&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>